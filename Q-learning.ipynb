{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-789353ef7b6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msimple_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtruck\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "import model\n",
    "import numpy as np\n",
    "import random\n",
    "from utils import simple_graph\n",
    "import tank\n",
    "import truck\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation, rc\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def initialize_system():\n",
    "    # Tanks' information\n",
    "    global n\n",
    "    n=5\n",
    "    tank_ids = list(range(1,n+1))\n",
    "    tank_max_loads =  np.array([100., 100., 200., 300., 400.])\n",
    "    tank_current_loads =  np.array([50., 60., 120., 150., 300.])\n",
    "    tank_consumption_rates =  np.array([10.] * n)\n",
    "    \n",
    "    global n_discrete_load_levels\n",
    "    n_discrete_load_levels = np.array([5,5,10,10,10])\n",
    "\n",
    "    # Trucks' information\n",
    "    global k\n",
    "    k=2\n",
    "    truck_ids = list(range(k))\n",
    "    truck_max_loads = np.array([20., 50.])\n",
    "    truck_current_loads = truck_max_loads.copy()\n",
    "    truck_current_positions =  np.array([0] * k)\n",
    "    #truck_fractions_deliverable =  np.array([1.] * k) # we for now we only allow to deliver all the content of the truck\n",
    "    truck_fractions_deliverable =  np.array([ np.array([0.5, 1.]), \n",
    "                                              np.array([0.5, 1.])\n",
    "                                            ]) # we for now we only allow to deliver all the content of the truck\n",
    "    global n_discrete_load_levels_trucks\n",
    "    n_discrete_load_levels_trucks = np.array([2,2])\n",
    "\n",
    "    # System's information\n",
    "    def simple_graph(n: int):    \n",
    "        A = np.zeros((n,n))\n",
    "        A[0,0:n] = 1\n",
    "        A = A.astype(int)\n",
    "        return(A)\n",
    "\n",
    "    graph = simple_graph(n+1)\n",
    "    tanks = [tank.Tank( tank_id, current_load, max_load, consumption_rate, n_lvls) \n",
    "             for  tank_id, current_load, max_load, consumption_rate, n_lvls in \n",
    "             zip( tank_ids, tank_current_loads, tank_max_loads, tank_consumption_rates, n_discrete_load_levels)]\n",
    "    trucks = [truck.Truck( truck_id, current_load, max_load, current_position, load_fractions_deliverable, n_lvls) \n",
    "             for  truck_id, current_load, max_load, current_position, load_fractions_deliverable, n_lvls in \n",
    "             zip(truck_ids, truck_current_loads, truck_max_loads, truck_current_positions, \n",
    "                 truck_fractions_deliverable, n_discrete_load_levels_trucks)]\n",
    "\n",
    "    def simple_weights(n: int, w: float):    \n",
    "        W = np.full((n,n), np.inf)\n",
    "        W[0,:] = w\n",
    "        return(W)\n",
    "    w =  np.array([0, 20., 10., 30., 50.5, 45.])\n",
    "\n",
    "    weights_matrix = simple_weights(n+1, w)\n",
    "    \n",
    "    return(tanks, trucks, graph, weights_matrix)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tanks, trucks, graph, weights_matrix = initialize_system()\n",
    "toy_system = model.System(tanks = tanks, trucks = trucks, adjacency_matrix = graph, weights_matrix = weights_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation\n",
    "%matplotlib inline\n",
    "\n",
    "seed = None \n",
    "\n",
    "iterations = 50\n",
    "visualization_steps = []\n",
    "rewards_list = []\n",
    "\n",
    "for it in range(iterations):\n",
    "    rewards = toy_system.random_action(seed);\n",
    "    rewards_list.append(rewards)\n",
    "    #print(toy_system.state())\n",
    "    print(\"Final rewards: \", rewards);\n",
    "    toy_system.reset_trucks_positions();\n",
    "    visualization_steps.append(toy_system.visualize());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization_steps[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization_steps[0][0]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "visualization_steps;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_system_animation(visualization_steps, n_iterations, skip = 1):\n",
    "    \n",
    "    def barlist(n, visualization_steps = visualization_steps): \n",
    "        return visualization_steps[n][2]\n",
    "\n",
    "    fig=plt.figure()\n",
    "\n",
    "    N=int(n_iterations/skip) #Number of frames\n",
    "    x=visualization_steps[0][0]+1\n",
    "\n",
    "    plt.bar(x,visualization_steps[0][1], color = 'black')\n",
    "    barcollection = plt.bar(x,barlist(0), color = 'blue')\n",
    "\n",
    "    def animate(i):\n",
    "        y=barlist(i+1)\n",
    "        for i, b in enumerate(barcollection):\n",
    "            b.set_height(y[i])\n",
    "\n",
    "    anim=animation.FuncAnimation(fig,animate,repeat=False,blit=False,frames=N-1,\n",
    "                                 interval=100)\n",
    "    return(anim)\n",
    "    #anim.save('mymovie.mp4',writer=animation.FFMpegWriter(fps=10))\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#toy_system.visualize_step(visualization_steps[2]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.plot(range(iterations),rewards_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_system.n_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_s = ((n+1)**k)\n",
    "a = np.prod(n_discrete_load_levels)\n",
    "b = np.prod(n_discrete_load_levels_trucks)\n",
    "n_s = n_s *a *b\n",
    "n_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Small system to apply Q-learning algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_system.graph[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_system.discrete_state()\n",
    "\n",
    "\n",
    "\n",
    "rewards = toy_system.random_action(seed);\n",
    "rewards_list.append(rewards)\n",
    "#prnt(toy_system.state())\n",
    "print(\"Final rewards: \", rewards);\n",
    "toy_system.reset_trucks_positions();\n",
    "\n",
    "toy_system.da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_system.ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = [[3, 4], [1, 1], [0, 0, 1, 0, 1]]\n",
    "aa= ''.join(str(''.join(str(y) for y in x)) for x in a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(toy_system.ds)\n",
    "print(toy_system.da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa_new = toy_system.state_action_to_string()\n",
    "sa_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_dict = {}\n",
    "new_dict[sa_new] = {\"qval\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dict[sa_new]['qval']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Off-policy algorithm (Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate0 = 0.05\n",
    "learning_rate_decay = 0.1\n",
    "n_iterations = 2*10**6\n",
    "discount_rate = 0.8\n",
    "\n",
    "tanks, trucks, graph, weights_matrix = initialize_test_system()\n",
    "toy_system = model.System(tanks = tanks, trucks = trucks, adjacency_matrix = graph, weights_matrix = weights_matrix)\n",
    "\n",
    "Q = {}\n",
    "\n",
    "def is_key(dic, key):\n",
    "    if key in dic:\n",
    "        return(True)\n",
    "    else: return(False)\n",
    "\n",
    "is_key(new_dict, sa_new)\n",
    "\n",
    "state_length = 2*k + n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_obj(obj, name ):\n",
    "    with open('obj/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open('obj/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# \"Train zone\". Q-values are being converging to the optimal, theoretically\n",
    "\n",
    "train = False\n",
    "if train:\n",
    "\n",
    "    visualization_steps = []\n",
    "    rewards_list = []\n",
    "\n",
    "    verbose = False\n",
    "    verbose_info = True\n",
    "\n",
    "    for iteration in range(n_iterations):\n",
    "        toy_system.update_state() \n",
    "\n",
    "        if verbose: print(\"System state before, \", toy_system.ds)\n",
    "\n",
    "        reward = toy_system.random_action(seed = None, verbose = verbose)\n",
    "        sa_current = toy_system.state_action_to_string()\n",
    "\n",
    "        if is_key(Q, sa_current) == False:\n",
    "            Q[sa_current] = 0\n",
    "\n",
    "        learning_rate = learning_rate0 / (1+iteration*learning_rate_decay)\n",
    "\n",
    "        Q_max = max([Q[key] for key in Q.keys() if key.startswith(sa_current[0:state_length])])\n",
    "        \n",
    "        if Q[sa_current] != -np.inf:\n",
    "            Q[sa_current] = ( (1-learning_rate) * Q[sa_current] \n",
    "                             + learning_rate* (reward + discount_rate * Q_max)\n",
    "                            )\n",
    "        if verbose:\n",
    "            print(\"System state after, \", toy_system.ds)\n",
    "            print(\"System action after, \", toy_system.da)\n",
    "            print(\"sa current, \", sa_current)\n",
    "\n",
    "        toy_system.reset_trucks_positions();\n",
    "        toy_system.reset_trucks_loads();\n",
    "\n",
    "        if iteration % 10000 == 0:\n",
    "            print(\"Iteration \", iteration)\n",
    "            if verbose_info:\n",
    "                print(\"s, a\", toy_system.s, toy_system.a)\n",
    "                print(\"ds, da\", toy_system.ds, toy_system.da)\n",
    "                \n",
    "            #Save visualization and rewards\n",
    "            rewards_list.append(rewards);\n",
    "            visualization_steps.append(toy_system.visualize());    \n",
    "\n",
    "            save_obj(Q, \"Q-dict-test-\" + f\"{iteration}\")   \n",
    "            save_obj(visualization_steps, \"vis/vis-train-test-\" + f\"{iteration}\")   \n",
    "            save_obj(rewards_list, \"rewards/rew-train-test-\" + f\"{iteration}\")   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    save_obj(Q, \"Q-dict-test-\" + f\"{iteration}\")   \n",
    "    save_obj(visualization_steps, \"vis-train-test-\" + f\"{iteration}\")   \n",
    "    save_obj(rewards_list, \"rew-train-test-\" + f\"{iteration}\")   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(0.1)*np.inf\n",
    "#[key for key in list(Q) if key.startswith('5')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optimal policy using the Q values trained previously:\n",
    "# Q = load_obj(\"Q-dict-test-\"+f'{n_iterations-1}')\n",
    "# #print(Q.keys())\n",
    "\n",
    "# rewards_l = load_obj('rewards/rew-train-test'+f'{n_iterations}')\n",
    "# vis_l = load_obj('vis/vis-train-test-'+f'{n_iterations}')\n",
    "\n",
    "# anim = create_system_animation(vis_l, n_iterations)\n",
    "# HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "new_keys = new_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#[Q[key] for key in new_keys if key.startswith(sa_new[0:2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_system.ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "toy_system.da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[1,2,3][0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Q['3311-1-1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_toy_system = model.System(tanks = tanks, trucks = trucks, adjacency_matrix = graph, weights_matrix = weights_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To do\n",
    "\n",
    "1. Función que permita aplicar una acción al sistema usando el código de números enteros (como en las keys de los dicts)\n",
    "\n",
    "2. Hacer una simulación que vaya aplicando, en cada estado, la accion con mayor Q-value en el diccionario Q.\n",
    "\n",
    "3. Visualizar como evoluciona el sistema con la optimal policy estimada con alguna random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_toy_system.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = '1234'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in a: \n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing deterministic_action function from System class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tanks, trucks, graph, weights_matrix = initialize_test_system()\n",
    "test_toy_system = model.System(tanks = tanks, trucks = trucks, adjacency_matrix = graph, weights_matrix = weights_matrix)\n",
    "test_toy_system.ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test example:\n",
    "# System state before,  [[0, 0], [1, 1], [0, 0, 1, 0, 1]]\n",
    "# System state after,  [[0, 0], [1, 1], [0, 0, 1, 0, 1]]\n",
    "# System action after,  [[3, 0], [0, 1]]\n",
    "# sa current,  0011001013001\n",
    "\n",
    "s0 = '001100101'\n",
    "a0 = '3001'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int('1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_toy_system.set_discrete_state(s0)\n",
    "test_toy_system.ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_toy_system.deterministic_action(a0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "toy_system.update_state() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_toy_system.da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_toy_system.ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "200000/3600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize simulation of 900 train steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "verbose_vis = False\n",
    "\n",
    "if verbose_vis:\n",
    "\n",
    "    n_iterations = 900\n",
    "\n",
    "    Q = load_obj(\"Q-dict-test-\"+f'{n_iterations}')\n",
    "    #print(Q.keys())\n",
    "\n",
    "    rewards_l = load_obj('rewards/rew-train-test-'+f'{n_iterations}')\n",
    "    vis_l = load_obj('vis/vis-train-test-'+f'{n_iterations}')\n",
    "\n",
    "    anim = create_system_animation(vis_l, n_iterations)\n",
    "    HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#each 9 frames\n",
    "if verbose_vis:\n",
    "    anim = create_system_animation(vis_l, n_iterations, 9)\n",
    "    HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Off-policy algorithm (Optimal policy from the trained Q-values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize system\n",
    "tanks, trucks, graph, weights_matrix = initialize_test_system()\n",
    "\n",
    "# Load trained Q-values\n",
    "n_iterations = 900\n",
    "Q = load_obj(\"Q-dict-test-\"+f'{n_iterations}')\n",
    "\n",
    "test_toy_system = model.System(tanks = tanks, trucks = trucks, adjacency_matrix = graph, weights_matrix = weights_matrix)\n",
    "a0 = '3001'\n",
    "s0 = test_toy_system.state_to_string()\n",
    "print(s0)\n",
    "test_toy_system.deterministic_action(a0)\n",
    "\n",
    "print(test_toy_system.ds)\n",
    "print(test_toy_system.state_to_string())\n",
    "print(test_toy_system.da)\n",
    "print(test_toy_system.action_to_string())\n",
    "\n",
    "print(test_toy_system.state_action_to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Way to access to the keys of the dictionary Q\n",
    "key_index = 0\n",
    "i = list(Q)[key_index]\n",
    "#Q value:\n",
    "Q[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0_keys = [key for key in list(Q) if key.startswith(s0)][1:10]\n",
    "s0_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0_q = [Q[s0_key] for s0_key in s0_keys]\n",
    "print(s0_q)\n",
    "print(s0_q == max(s0_q))\n",
    "optimal_key_index = np.where(np.isin(s0_q, max(s0_q)))[0][0]\n",
    "optimal_key = s0_keys[optimal_key_index]\n",
    "print(optimal_key)\n",
    "\n",
    "optimal_action = optimal_key[toy_system.state_length:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Given a state, returns the action that has the highest Q-value.\n",
    "\n",
    "def optimal_policy(state, Q):\n",
    "    \"\"\"\n",
    "    state must be in the string-integers code\n",
    "    \"\"\"\n",
    "    state_keys = [key for key in list(Q) if key.startswith(s0)]\n",
    "    if len(state_keys) == 0:\n",
    "        return(None)\n",
    "    \n",
    "    state_q = [Q[state_key] for state_key in state_keys]\n",
    "    \n",
    "    #print(\"state_q \", state_q[1:min(10,len(state_q))])\n",
    "    \n",
    "    max_q = max(state_q)\n",
    "    #print(\"max_q\", max_q)\n",
    "    optimal_key_index = np.where(np.isin(state_q, max_q ))[0][0]\n",
    "   #print(\"optimal_key_index\", optimal_key_index)\n",
    "    optimal_key = state_keys[optimal_key_index]\n",
    "    #print(\"optimal_key\", optimal_key)\n",
    "    \n",
    "    optimal_action = optimal_key[toy_system.state_length:]\n",
    "    \n",
    "    return(optimal_action)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 's0' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-5340bc398d65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moptimal_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 's0' is not defined"
     ]
    }
   ],
   "source": [
    "optimal_policy(s0, Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test optimal policy with a small simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'initialize_test_system' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-30276813cc28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Initialize system\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtanks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrucks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialize_test_system\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Load trained Q-values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mn_iterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1999999\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'initialize_test_system' is not defined"
     ]
    }
   ],
   "source": [
    "# Initialize system\n",
    "tanks, trucks, graph, weights_matrix = initialize_test_system()\n",
    "\n",
    "# Load trained Q-values\n",
    "n_iterations = 1999999\n",
    "Q = load_obj(\"Q-dict-test-\"+f'{n_iterations}')\n",
    "\n",
    "\n",
    "# 20k\n",
    "#Q = load_obj(\"Q-dict-test-20k\")\n",
    "#n_iterations = 20000\n",
    "\n",
    "sim_iterations = 1000\n",
    "test_toy_system = model.System(tanks = tanks, trucks = trucks, adjacency_matrix = graph, weights_matrix = weights_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = True\n",
    "visualization_steps = []\n",
    "rewards_list = []\n",
    "\n",
    "\n",
    "if test:\n",
    "    for i in range(sim_iterations):\n",
    "        #print(\"state\", test_toy_system.s, test_toy_system.ds)\n",
    "        test_toy_system.update_state()\n",
    "        visualization_steps.append(test_toy_system.visualize());\n",
    "    \n",
    "        s0 = test_toy_system.state_to_string()\n",
    "        best_action = optimal_policy(s0, Q)\n",
    "        #print(\"best_action\", best_action)\n",
    "        \n",
    "        if best_action == None:\n",
    "            print(\"Random action is performed, because the current state is unknown for the trained Q\")\n",
    "            test_toy_system.random_action()\n",
    "        \n",
    "        else:\n",
    "            reward = test_toy_system.deterministic_action(best_action)\n",
    "            print(i,reward, best_action)\n",
    "        \n",
    "        test_toy_system.reset_trucks_positions();\n",
    "        test_toy_system.reset_trucks_loads();\n",
    "\n",
    "        \n",
    "        \n",
    "        #Save visualization and rewards\n",
    "        rewards_list.append(reward);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_l = rewards_list\n",
    "vis_l = visualization_steps\n",
    "\n",
    "anim = create_system_animation(vis_l, sim_iterations,1)\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max(-np.inf,-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Q['001101101'+ '0010']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot([i for i in range(len(rewards_list))],rewards_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = -np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a == -np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len([key for key in list(Q) if key.startswith(s0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.linspace(0,3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#list(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "old_position = 5\n",
    "possible_positions_index = np.isin(test_toy_system.graph[old_position], 1)\n",
    "possible_positions = np.where(possible_positions_index)\n",
    "possible_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_position = random.randrange(len(possible_positions_index)-1 )\n",
    "new_position\n",
    "len(possible_positions_index)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "1474560/(n+1)**k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " 2*10**6"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import model\n",
    "import numpy as np\n",
    "import random\n",
    "import tank\n",
    "import truck\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib import animation, rc\n",
    "from IPython.display import HTML\n",
    "\n",
    "import utilsq as ut\n",
    "import functions\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "368640"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def initialize_test_system(seed = None):\n",
    "    if seed != None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    # Tanks' information\n",
    "    global n\n",
    "    n = 5 \n",
    "    tank_ids = list(range(1,n+1))\n",
    "    tank_max_loads =  np.array([100., 200, 100., 800., 200.])\n",
    "    #tank_current_loads =  np.array([50., 60., 120., 150., 300.])\n",
    "    #tank_current_loads = tank_max_loads.copy()\n",
    "    tank_current_loads = np.full(n,0)\n",
    "    tank_consumption_rates =  np.array([5.] * n)\n",
    "    \n",
    "    global n_discrete_load_levels\n",
    "    n_discrete_load_levels = np.array([4,4,4,4,4])\n",
    "    \n",
    "    load_level_percentages = np.array([ #b , c, e\n",
    "                                                [0.2, 0.61, 0.9],\n",
    "                                                [0.01, 0.03, 0.9],\n",
    "                                                [0.05, 0.16, 0.9],\n",
    "                                                [0.11, 0.33, 0.85],\n",
    "                                                [0.09, 0.26, 0.9]\n",
    "                                               ])\n",
    "        \n",
    "    for i, (lvl, max_load) in enumerate(zip(n_discrete_load_levels, tank_max_loads)):\n",
    "        a = np.linspace(0,max_load, lvl+1)[1]\n",
    "        current_load = np.random.randint(a+1,max_load)\n",
    "        tank_current_loads[i] = current_load  \n",
    "\n",
    "    # Trucks' information\n",
    "    global k\n",
    "    k = 2\n",
    "    truck_ids = list(range(k))\n",
    "    truck_max_loads = np.array([25.,40.])\n",
    "    truck_current_loads = truck_max_loads.copy()\n",
    "    truck_current_positions =  np.array([5] * k)\n",
    "    #truck_fractions_deliverable =  np.array([1.] * k) # we for now we only allow to deliver all the content of the truck\n",
    "    truck_fractions_deliverable =  np.array([ np.array([1.]), \n",
    "                                              np.array([1.])\n",
    "                                            ]) # we for now we only allow to deliver all the content of the truck\n",
    "    global n_discrete_load_levels_trucks\n",
    "    n_discrete_load_levels_trucks = np.array([1,1])\n",
    "\n",
    "    # System's information\n",
    "   \n",
    "    graph = ut.simple_graph(n+1)\n",
    "    tanks = [tank.Tank( tank_id, current_load, max_load, consumption_rate, n_lvls, d_lvls) \n",
    "             for  tank_id, current_load, max_load, consumption_rate, n_lvls, d_lvls in \n",
    "             zip( tank_ids, tank_current_loads, tank_max_loads, tank_consumption_rates, n_discrete_load_levels,\n",
    "                  load_level_percentages)]\n",
    "    trucks = [truck.Truck( truck_id, current_load, max_load, current_position, load_fractions_deliverable, n_lvls) \n",
    "             for  truck_id, current_load, max_load, current_position, load_fractions_deliverable, n_lvls in \n",
    "             zip(truck_ids, truck_current_loads, truck_max_loads, truck_current_positions, \n",
    "                 truck_fractions_deliverable, n_discrete_load_levels_trucks)]\n",
    "\n",
    "    #w =  np.array([0, 20., 10., 30., 50.5, 45.])\n",
    "    w =  np.array([32., 159., 162., 156.,156., 0.])\n",
    "\n",
    "    weights_matrix = ut.simple_weights(n+1, w)\n",
    "    \n",
    "    return(tanks, trucks, graph, weights_matrix)\n",
    "\n",
    "tanks, trucks, graph, weights_matrix = initialize_test_system()\n",
    "toy_system = model.System(tanks = tanks, trucks = trucks, adjacency_matrix = graph, weights_matrix = weights_matrix)\n",
    "\n",
    "#print(toy_system.weights)\n",
    "\n",
    "a_s_dim = toy_system.states_dim * toy_system.actions_dim\n",
    "a_s_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[77, 143, 40, 307, 122]\n",
      "[[0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [1 1 1 1 1 1]]\n",
      "[[  inf   inf   inf   inf   inf   inf]\n",
      " [  inf   inf   inf   inf   inf   inf]\n",
      " [  inf   inf   inf   inf   inf   inf]\n",
      " [  inf   inf   inf   inf   inf   inf]\n",
      " [  inf   inf   inf   inf   inf   inf]\n",
      " [  32.  159.  162.  156.  156.    0.]]\n"
     ]
    }
   ],
   "source": [
    "tanks, trucks, graph, weights_matrix = initialize_test_system(seed = 42)\n",
    "system = model.System(tanks = tanks, trucks = trucks, adjacency_matrix = graph, weights_matrix = weights_matrix)\n",
    "print(system.tank_loads())\n",
    "#print([load for load in system.tank_loads])\n",
    "print(system.graph)\n",
    "print(system.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.2 ,  0.61,  0.9 ])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tanks[0].level_percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[77, 143, 40, 307, 122]\n",
      "[50.0, 182.0, 82.0, 457.0, 72.0]\n",
      "[72.0, 180.0, 98.0, 594.0, 109.0]\n"
     ]
    }
   ],
   "source": [
    "verbose = False\n",
    "print(system.tank_loads())\n",
    "def reinitialize_system(system, seed = None):\n",
    "    if seed != None:\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "    for tank, tank_levels in zip(system.tanks, system.tanks_level):\n",
    "        a = tank_levels[0]\n",
    "        b = tank_levels[-1]\n",
    "        current_load = np.random.randint(a+1,b)*1.0\n",
    "        tank.load = current_load\n",
    "    system.reset_trucks_positions(); \n",
    "    return(system)    \n",
    "\n",
    "system = reinitialize_system(system,3)\n",
    "print(system.tank_loads())\n",
    "system = reinitialize_system(system,4)\n",
    "print(system.tank_loads())\n",
    "reward = system.random_action(seed = 1, verbose = verbose)\n",
    "system.reset_trucks_positions();\n",
    "#print(reward)\n",
    "reward = system.random_action(seed = 1, verbose = verbose)\n",
    "#print(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning algorithm (off-policy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train parameters:\n",
    "retrain = False\n",
    "train_epsilon = True\n",
    "\n",
    "learning_rate0 = 0.05 #??\n",
    "learning_rate_decay = 0.01 #??\n",
    "\n",
    "episodes = 10*10**6 #episodes\n",
    "episodes_epsilon_min = 5*10**5\n",
    "train_freq = 10**2 # 10**4\n",
    "episode_length = 30\n",
    "\n",
    "discount_rate = 1\n",
    "\n",
    "epsilon0 = 1.0\n",
    "epsilon_decay =( 1./(episodes_epsilon_min/2.0) ) *10\n",
    "epsilon_min = 0.05\n",
    "\n",
    "verbose = False\n",
    "verbose_info = False\n",
    "\n",
    "seed = 42\n",
    "\n",
    "train_visualization_steps = []\n",
    "train_rewards_list = []\n",
    "\n",
    "tanks, trucks, graph, weights_matrix = initialize_test_system()\n",
    "toy_system = model.System(tanks = tanks, trucks = trucks, adjacency_matrix = graph, weights_matrix = weights_matrix)\n",
    "\n",
    "Q = {}\n",
    "\n",
    "simulation_id = 14\n",
    "\n",
    "# if retrain == True:\n",
    "#     simulation_id_retrain = 3\n",
    "#     iteration_retrain = 50*10**6\n",
    "#     Q = ut.load_obj(\"Q-dict-sim\" + f\"{simulation_id_retrain}\" + \"-\" + f\"{iteration_retrain}\")\n",
    "\n",
    "ut.save_obj(toy_system, \"system-sim\"+f\"{simulation_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def episodic_train_Q_epsilon( \n",
    "            epsilon0 = epsilon0,\n",
    "            epsilon_min = epsilon_min,\n",
    "            n_episodes = episodes, \n",
    "            episode_length = episode_length,\n",
    "            learning_rate0 = learning_rate0,\n",
    "            learning_rate_decay = learning_rate_decay,\n",
    "            discount_rate = discount_rate,\n",
    "            system = toy_system,\n",
    "            Q = Q, verbose = verbose, verbose_info = verbose_info,\n",
    "            visualization_steps = train_visualization_steps, rewards_list = train_rewards_list,\n",
    "            seed = seed, \n",
    "            freq = train_freq,\n",
    "            simulation_id = simulation_id,\n",
    "            round_time = 2\n",
    "           ):\n",
    "    \n",
    "    time_start = time.time()\n",
    "    \n",
    "    for episode in range(1,n_episodes+1):\n",
    "        reinitialize_system(system, seed = episode)\n",
    "        \n",
    "        ### epsilon-greedy exploration\n",
    "        epsilon = max( epsilon_min, epsilon0 / (1+(episode-1)*epsilon_decay) ) \n",
    "        \n",
    "        ### decrement of learning rate\n",
    "        learning_rate = learning_rate0 / (1+(episode-1)*learning_rate_decay)        \n",
    "\n",
    "        discounted_reward = 0\n",
    "        \n",
    "        for t in range(episode_length):\n",
    "\n",
    "            system.update_state()\n",
    "            s_current = system.state_to_string()                \n",
    "            p = np.random.uniform()\n",
    "\n",
    "            if p > epsilon:\n",
    "                #DETERMINISTIC ACTION OPTIMAL\n",
    "                s0 = system.state_to_string()\n",
    "                best_action = optimal_policy(s0, Q)\n",
    "                if best_action == None:\n",
    "                    reward = system.random_action(seed = (seed + (t+1)*episode), verbose = verbose)\n",
    "                else:\n",
    "                    reward = system.deterministic_action(best_action)\n",
    "                #print(best_action)\n",
    "            else:\n",
    "                reward = system.random_action(seed = (seed + (t+1)*episode), verbose = verbose)\n",
    "\n",
    "            a_current = system.action_to_string()\n",
    "            sa_current = ''.join([s_current, a_current])\n",
    "\n",
    "            system.update_state()\n",
    "            sa_new = system.state_action_to_string()\n",
    "\n",
    "            if ut.is_key(Q, sa_current) == False:\n",
    "                Q[sa_current] = 0\n",
    "\n",
    "            Q_max = max([Q[key] for key in Q.keys() if key.startswith(sa_new[0:system.state_length])]+[0.0]) \n",
    "\n",
    "            if Q[sa_current] != -np.inf:\n",
    "                Q[sa_current] = ( (1-learning_rate) * Q[sa_current] \n",
    "                                 + learning_rate* (reward + discount_rate * Q_max)\n",
    "                                )\n",
    "                \n",
    "            discounted_reward = discounted_reward + (discount_rate**t) * reward\n",
    "            system.reset_trucks_positions();     \n",
    "            system.reset_trucks_loads();\n",
    "            \n",
    "        rewards_list.append(discounted_reward);\n",
    "        if episode % freq == 0:\n",
    "                time_end = time.time()\n",
    "                print(\"Episode \", episode, \", Elapsed time \", round( (time_end-time_start)/60., round_time), \" minuts.\",\n",
    "                      \"epsilon\", round(epsilon,4), \n",
    "                     \"Discounted reward: \", discounted_reward)\n",
    "                \n",
    "                if verbose_info:\n",
    "                    print(\"s, a\", system.s, system.a)\n",
    "                    print(\"ds, da\", system.ds, system.da)\n",
    "\n",
    "                #Save visualization and rewards\n",
    "                visualization_steps.append(toy_system.visualize());    \n",
    "\n",
    "                ut.save_obj(Q, \"Q-dict-sim\" + f\"{simulation_id}\" + \"-\" + f\"{episode}\")   \n",
    "                ut.save_obj(visualization_steps, \"vis/vis-train-sim\" + f\"{simulation_id}\" + \"-\" + f\"{episode}\")   \n",
    "                #rewards_list.append(discounted_reward);\n",
    "                ut.save_obj(rewards_list, \"discrewards/discrew-train-sim\" + f\"{simulation_id}\" + \"-\" + f\"{episode}\")\n",
    "   \n",
    "    end_time = round(time.time()-time_start,round_time)        \n",
    "    print(f\"Training finished. Total episodes: {n_episodes}. Elapsed time: {round(end_time/60., round_time)} minuts.\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Given a state, returns the action that has the highest Q-value.\n",
    "\n",
    "def optimal_policy(state, Q, system = toy_system):\n",
    "    \"\"\"\n",
    "    state must be in the string-integers code\n",
    "    \"\"\"\n",
    "    state_keys = [key for key in list(Q) if key.startswith(state)]\n",
    "    if len(state_keys) == 0:\n",
    "        return(None)\n",
    "    \n",
    "    state_q = [Q[state_key] for state_key in state_keys]\n",
    "    \n",
    "    #print(\"state_q \", state_q[1:min(10,len(state_q))])\n",
    "    \n",
    "    max_q = max(state_q)\n",
    "    #print(\"max_q\", max_q)\n",
    "    optimal_key_index = np.where(np.isin(state_q, max_q ))[0][0]\n",
    "    #print(\"optimal_key_index\", optimal_key_index)\n",
    "    optimal_key = state_keys[optimal_key_index]\n",
    "    #print(\"optimal_key\", optimal_key)\n",
    "    optimal_action = optimal_key[system.state_length:]\n",
    "    \n",
    "    return(optimal_action)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  100 , Elapsed time  0.04  minuts. epsilon 0.9961 Discounted reward:  -6913.90836108\n",
      "Episode  200 , Elapsed time  0.11  minuts. epsilon 0.9921 Discounted reward:  -5554.25339019\n",
      "Episode  300 , Elapsed time  0.19  minuts. epsilon 0.9882 Discounted reward:  -7548.63598863\n",
      "Episode  400 , Elapsed time  0.27  minuts. epsilon 0.9843 Discounted reward:  -6910.59713875\n",
      "Episode  500 , Elapsed time  0.36  minuts. epsilon 0.9804 Discounted reward:  -6363.72203443\n",
      "Episode  600 , Elapsed time  0.46  minuts. epsilon 0.9766 Discounted reward:  -6703.72061218\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-c0c85facdef9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtrain_epsilon\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m#train_Q_epsilon()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mepisodic_train_Q_epsilon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-9bbd36eb9913>\u001b[0m in \u001b[0;36mepisodic_train_Q_epsilon\u001b[0;34m(epsilon0, epsilon_min, n_episodes, episode_length, learning_rate0, learning_rate_decay, discount_rate, system, Q, verbose, verbose_info, visualization_steps, rewards_list, seed, freq, simulation_id, round_time)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msa_current\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mQ_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msa_new\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_length\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msa_current\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-9bbd36eb9913>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msa_current\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mQ_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msa_new\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_length\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msa_current\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if train_epsilon == True:\n",
    "    #train_Q_epsilon()\n",
    "    episodic_train_Q_epsilon()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST PARAMETERS AND INITIALIZATION\n",
    "\n",
    "# Initialize system\n",
    "tanks, trucks, graph, weights_matrix = initialize_test_system(seed = 42)\n",
    "test_toy_system = model.System(tanks = tanks, trucks = trucks, adjacency_matrix = graph, weights_matrix = weights_matrix)\n",
    "\n",
    "#Load trained Q-values\n",
    "# if train == False:\n",
    "#     simulation_id = 8\n",
    "#     train_iterations = 20*10**5\n",
    "#     test_toy_system = ut.load_obj(\"system-sim\"+ f\"{simulation_id}\")\n",
    "#     Q = ut.load_obj(\"Q-dict-sim\" + f\"{simulation_id}\" + \"-\" + f\"{train_iterations}\")\n",
    "\n",
    "\n",
    "# if retrain == False:\n",
    "#     simulation_id = 7\n",
    "#     train_iterations = 259*10**6\n",
    "#     test_toy_system = ut.load_obj(\"system-sim\"+ f\"{simulation_id}\")    \n",
    "#     Q = ut.load_obj(\"Q-dict-sim\" + f\"{simulation_id}\" + \"-\" + f\"{train_iterations}\")\n",
    "\n",
    "   \n",
    "#test_toy_system = ut.load_obj(\"system-sim\"+ f\"{simulation_id}\")    \n",
    "if train_epsilon == False:\n",
    "    simulation_id = 13\n",
    "    episodes =500000\n",
    "    \n",
    "Q = ut.load_obj(\"Q-dict-sim\" + f\"{simulation_id}\" + \"-\" + f\"{episodes}\")\n",
    "\n",
    "\n",
    "test_episodes = 10\n",
    "episode_length =50\n",
    "test_freq = 1\n",
    "test_verbose = False\n",
    "\n",
    "test_visualization_steps = []\n",
    "test_rewards_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_Q(n_episodes = test_episodes, \n",
    "           episode_length = episode_length,\n",
    "           system = test_toy_system,\n",
    "           visualization_steps = test_visualization_steps, \n",
    "           rewards_list = test_rewards_list,\n",
    "           freq = test_freq,\n",
    "           test_verbose = test_verbose\n",
    "           \n",
    "          ):\n",
    "    \n",
    "    for episode in range(1,n_episodes+1): \n",
    "        reinitialize_system(system, seed = episode)\n",
    "\n",
    "        discounted_reward = 0      \n",
    "        \n",
    "        for i in range(1,episode_length+1):\n",
    "            #print(\"state\", test_toy_system.s, test_toy_system.ds)\n",
    "            system.update_state()\n",
    "\n",
    "            #Save visualization steps\n",
    "            if i % freq == 0:\n",
    "                visualization_steps.append(system.visualize());\n",
    "\n",
    "            s0 = system.state_to_string()\n",
    "            best_action = optimal_policy(s0, Q)\n",
    "            #print(\"best_action\", best_action)\n",
    "\n",
    "            if best_action == None:\n",
    "                reward = system.random_action()\n",
    "                if i % freq == 0:\n",
    "                    if test_verbose == True:\n",
    "                        print(\"Episode\", episode, \"t\", i-1, reward, \" Random action is performed. Current state unknown for Q.\")\n",
    "\n",
    "            else:\n",
    "                reward = system.deterministic_action(best_action)\n",
    "                if i % freq == 0:\n",
    "                    if test_verbose == True:\n",
    "                        print(\"Episode\", episode, \"t\",i-1,reward, best_action)\n",
    "\n",
    "            system.reset_trucks_positions();\n",
    "            system.reset_trucks_loads();\n",
    "            \n",
    "            discounted_reward = discounted_reward + (discount_rate**(i-1)) * reward\n",
    "\n",
    "        system.reset_trucks_positions();\n",
    "        \n",
    "        #Save rewards\n",
    "        if episode % freq == 0:\n",
    "            rewards_list.append(discounted_reward);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_Q()\n",
    "print(np.mean(test_rewards_list) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "plt.hist(test_rewards_list, bins = 20)\n",
    "plt.show()\n",
    "#test_rewards_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualizing test simulation:\n",
    "test_anim = ut.create_system_animation(test_visualization_steps, test_episodes * episode_length,test_freq)\n",
    "HTML(test_anim.to_html5_video())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p = plt.plot([i for i in range(len(test_rewards_list))], test_rewards_list)\n",
    "print(np.mean(test_rewards_list) )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualizing train simulation:\n",
    "\n",
    "# train_anim = ut.create_system_animation(train_visualization_steps, train_iterations, train_freq)\n",
    "# HTML(train_anim.to_html5_video())\n",
    "\n",
    "episode = 500000\n",
    "simulation_id = 13\n",
    "step = 1000\n",
    "discrewards_list = ut.load_obj(\"discrewards/discrew-train-sim\" + f\"{simulation_id}\" + \"-\" + f\"{episode}\")\n",
    "\n",
    "discrewards_list2 = [discrewards_list[i] for i in range(0,len(discrewards_list),step)]\n",
    "\n",
    "p = plt.plot([i for i in range(0,len(discrewards_list),step)], \n",
    "             discrewards_list2)\n",
    "\n",
    "#print(discrewards_list) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   # simulation_id = 4\n",
    "    #train_iterations = 2*10**6\n",
    "    #test_toy_system = ut.load_obj(\"system-sim\"+ f\"{simulation_id}\")    \n",
    "    #Q = ut.load_obj(\"Q-dict-sim\" + f\"{simulation_id}\" + \"-\" + f\"{train_iterations}\")\n",
    "\n",
    "len(list(Q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(range(0,len(discrewards_list),1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(system.tanks_level[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
